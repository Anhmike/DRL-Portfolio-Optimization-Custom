{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook determines the best set of hyperparameters to train the portfolio optimization model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import necessary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import boto3\n",
    "from IPython.display import Image\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.session import Session\n",
    "from sagemaker.tuner import IntegerParameter, CategoricalParameter, ContinuousParameter, HyperparameterTuner\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set local parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_type = 'cpu'\n",
    "instance_type = 'ml.m5.large'\n",
    "start_day = 2670\n",
    "n_instances = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compile environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "031118886020.dkr.ecr.us-east-1.amazonaws.com/portfolio-optimization-cpu:latest\n"
     ]
    }
   ],
   "source": [
    "role = get_execution_role()\n",
    "account = boto3.client('sts').get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "image_name = '{}.dkr.ecr.{}.amazonaws.com/portfolio-optimization-{}:latest'.format(account, region, image_type)\n",
    "print(image_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Set the objective\n",
    "Next we'll specify the objective metric that we'd like to tune and its definition, which includes the regular expression (Regex) needed to extract that metric from the CloudWatch logs of the training job. In this particular case, our script emits the objective metric directly, which we will 'Maximize'.  The actual objective metric $O$ is defined below.\n",
    "\n",
    "$\n",
    "\\qquad O = 100*(\\frac{P}{M} - 1) - max(0, t-60) \\\\\n",
    "\\qquad \\qquad \\text{where:} \\\\\n",
    "\\qquad \\qquad \\qquad P = \\text{The final Portfolio value} \\\\\n",
    "\\qquad \\qquad \\qquad M = \\text{The final Market value} \\\\\n",
    "\\qquad \\qquad \\qquad t = \\text{The total training time in minutes} \\\\\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_metric_name = 'Trading Performance'\n",
    "objective_type = 'Maximize'\n",
    "metric_definitions = [{'Name': objective_metric_name,\n",
    "                       'Regex': '(\\S+) training objective.'}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the hyperparameters to optimize\n",
    "The following hyperparameters are read from the command line in [train.py](container/src/train.py).\n",
    "\n",
    "| Name            | Type  | Default  | Description                        |\n",
    "|-----------------|-------|----------|------------------------------------|\n",
    "| max_epochs      | int   |    2000  | max epochs per new trading day     |\n",
    "| days_per_epoch  | int   |      40  | days in each epoch                 |\n",
    "| start_day       | int   |     504  | day to begin training              |\n",
    "| window_length   | int   |       1  | CNN window length                  |\n",
    "| memory_strength | float |     2.0  | memory exponential gain            |\n",
    "| target          | float |    1.02  | target portfolio/market ratio      |\n",
    "| fc1             | int   |     128  | size of 1st hidden layer           |\n",
    "| fc2             | int   |      64  | size of 2bd hidden layer           |\n",
    "| lr_actor        | float | 0.000371 | initial learning rate for actor    |\n",
    "| lr_critic       | float |   0.0011 | initial learning rate for critic   |\n",
    "| batch_size      | int   |      256 | mini batch size                    |\n",
    "| buffer_size     | int   |   100000 | replay buffer size                 |\n",
    "| gamma           | float |     0.91 | discount factor                    |\n",
    "| tau             | float |   0.0072 | soft update of target parameters   |\n",
    "| sigma           | float |    0.013 | OU Noise standard deviation        |\n",
    "\n",
    "Any of these could be tuned but we will down select to limit the search.    \n",
    "\n",
    "The hyperparameter tunner allow the hyperparameters to be defined as one of the following types. \n",
    "- `CategoricalParameter(list)` Categorical parameters need to take one value from a discrete set. \n",
    "- `ContinuousParameter(min, max)` Continuous parameters can take any real number value between the minimum and maximum value.\n",
    "- `IntegerParameter(min, max)` Integer parameters can take any integer value between the minimum and maximum value.\n",
    "\n",
    "_Note, if possible, it's almost always best to specify a value as the least restrictive type. For example, tuning learning rate as a continuous value between 0.01 and 0.2 is likely to yield a better result than tuning as a categorical parameter with values 0.01, 0.1, 0.15, or 0.2. Sometimes a parameter is categorical to limit the search space._\n",
    "\n",
    "_Also parameters maybe group a sequence of optimization may be executed._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameter_ranges = {\n",
    "    'days_per_epoch': IntegerParameter(40, 252),\n",
    "    'fc1': IntegerParameter(256, 512),\n",
    "    'fc2': IntegerParameter(128, 256),\n",
    "    'lr_actor': ContinuousParameter(0.0001, 0.001),\n",
    "    'lr_critic': ContinuousParameter(0.0005, 0.005)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the base estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Parameter image_name will be renamed to image_uri in SageMaker Python SDK v2.\n"
     ]
    }
   ],
   "source": [
    "estimator = Estimator(role=role,\n",
    "                  train_instance_count=n_instances,\n",
    "                  train_instance_type=instance_type,\n",
    "                  image_name=image_name,\n",
    "                  hyperparameters={'start_day': start_day})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the hyperparameter tuner object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner = HyperparameterTuner(estimator,\n",
    "                            objective_metric_name,\n",
    "                            hyperparameter_ranges,\n",
    "                            metric_definitions,\n",
    "                            max_jobs=100,\n",
    "                            max_parallel_jobs=len(hyperparameter_ranges) + 1,\n",
    "                            objective_type=objective_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the hyperparameter tuning\n",
    "After the hyperprameter tuning job is created, you should be able to describe the tuning job to see its progress in the next step, and you can go to SageMaker console -> `Training` -> `Hyperparameter tuning jobs` to see the progresss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "......................................................"
     ]
    }
   ],
   "source": [
    "tuner.fit()\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_parameters = tuner.best_estimator().hyperparameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_name = tuner.best_training_job()\n",
    "print('\\nBest model = {}.'.format(best_name))\n",
    "for name in hyperparameter_ranges.keys():\n",
    "    print('\\t{} = {}'.format(name, best_parameters[name]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compare optimal hyperparameters to manually optimized values.\n",
    "| Name        | Type  | Default | Description                        |\n",
    "|-------------|-------|---------|------------------------------------|\n",
    "| fc1         | int   |     128 | size of 1st hidden layer           |\n",
    "| fc2         | int   |      64 | size of 2bd hidden layer           |\n",
    "| lr_actor    | float |   0.001 | initial learning rate for actor    |\n",
    "| lr_critic   | float |   0.001 | initial learning rate for critic   |\n",
    "| batch_size  | int   |     256 | mini batch size                    |\n",
    "| buffer_size | int   |  100000 | replay buffer size                 |\n",
    "| gamma       | float |     0.9 | discount factor                    |\n",
    "| tau         | float |   0.001 | soft update of target parameters   |\n",
    "| sigma       | float |    0.01 | OU Noise standard deviation        |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Review the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Copy and unpack the optimum result archive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sagemaker_session = Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "s3 = boto3.resource('s3')\n",
    "s3.Bucket(bucket).download_file('{}/output/output.tar.gz'.format(best_name), 'output.tar.gz')\n",
    "shutil.unpack_archive('output.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the optimial results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename='history.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference\n",
    "- [SageMaker Tuning Example](https://github.com/awslabs/amazon-sagemaker-examples/tree/master/hyperparameter_tuning/pytorch_mnist)\n",
    "- [How Hyperparameter Tuning Works](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-how-it-works.html)\n",
    "- [Tuner API](https://sagemaker.readthedocs.io/en/stable/api/training/tuner.html)\n",
    "- [Estimater API](https://sagemaker.readthedocs.io/en/stable/api/training/estimators.html#sagemaker.estimator.EstimatorBase)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "notice": "Copyright 2018 Amazon.com, Inc. or its affiliates. All Rights Reserved. Licensed under the Apache License, Version 2.0 (the \"License\"). You may not use this file except in compliance with the License. A copy of the License is located at http://aws.amazon.com/apache2.0/ or in the \"license\" file accompanying this file. This file is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.",
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
